# streamlit_app.py
"""
Streamlit ëŒ€ì‹œë³´ë“œ: 'ë§Œì•½ ë‚´ê°€ ì‚¬ëŠ” ê³³ì´ í•´ìˆ˜ë©´ ìƒìŠ¹ìœ¼ë¡œ ê°€ë¼ì•‰ëŠ”ë‹¤ë©´?' (í•œêµ­ì–´ UI)
- ëª©ì : ê³µì‹ ê³µê°œë°ì´í„°(ê¸€ë¡œë²ŒÂ·í•´ìˆ˜ë©´Â·í•´ìˆ˜ë©´ ì˜¨ë„ ë“±)ë¡œ ë¨¼ì € ëŒ€ì‹œë³´ë“œ ìƒì„± í›„,
       í”„ë¡¬í”„íŠ¸ Inputì— ì œê³µëœ 'í•œêµ­(êµ­ë¦½í•´ì–‘ì¡°ì‚¬ì›) 21ê°œ ê´€ì¸¡ì†Œ 1991-2020' ë°ì´í„°ë¥¼ ì‚¬ìš©í•´ ë³„ë„ ëŒ€ì‹œë³´ë“œ ìƒì„±.
- ë°ì´í„° ì¶œì²˜(ì£¼ìš”):
    NOAA Global/altimeter/tide datasets / Climate.gov / PSL: https://psl.noaa.gov/data/timeseries/month/SEALEVEL/  (NOAA PSL)
    NOAA Sea Level Rise tools / OCM: https://coast.noaa.gov/slrdata/
    Global sea level (DataHub mirror): https://datahub.io/core/sea-level-rise (raw CSV: /r/sea-level.csv)
    í•œêµ­(êµ­ë¦½í•´ì–‘ì¡°ì‚¬ì›) ë¶„ì„ ë³´ë„ìë£Œ (1991-2020, 21ê°œ ê´€ì¸¡ì†Œ): https://www.mof.go.kr/doc/ko/selectDoc.do?docSeq=44140
    íˆ¬ë°œë£¨ ê´€ë ¨ ì—°ì„¤(ì°¸ê³  ê¸°ì‚¬): https://www.theguardian.com/environment/2021/nov/08/tuvalu-minister-to-address-cop26-knee-deep-in-seawater-to-highlight-climate-crisis
- êµ¬í˜„ ê·œì¹™ ìš”ì•½:
    - í‘œì¤€í™”: date, value, group(optional)
    - ì „ì²˜ë¦¬: ê²°ì¸¡/í˜•ë³€í™˜/ì¤‘ë³µ ì œê±°/ë¯¸ë˜ ë°ì´í„°(ë¡œì»¬ ìì • ì´í›„) ì œê±°
    - ìºì‹±: @st.cache_data ì‚¬ìš©
    - CSV ë‹¤ìš´ë¡œë“œ ì œê³µ
    - API ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„ â†’ ì‹¤íŒ¨í•˜ë©´ ì˜ˆì‹œ ë°ì´í„°ë¡œ ìë™ ëŒ€ì²´(í™”ë©´ ì•ˆë‚´)
- ì£¼ì˜: ì´ ì•±ì€ Codespaces/ë¡œì»¬ ì–´ë””ì„œë‚˜ ì‹¤í–‰ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŒ.
"""

import streamlit as st
import pandas as pd
import numpy as np
import datetime
from io import StringIO, BytesIO
import requests
from requests.adapters import HTTPAdapter, Retry
import plotly.express as px
import plotly.graph_objects as go
import pdfplumber

# ------------------------
# ì„¤ì •: í•œêµ­ì–´ UI, í˜ì´ì§€
# ------------------------
st.set_page_config(page_title="í•´ìˆ˜ë©´ ìƒìŠ¹ ëŒ€ì‹œë³´ë“œ", layout="wide")
st.title("ğŸ“˜ ë³´ê³ ì„œ: ë§Œì•½ ë‚´ê°€ ì‚¬ëŠ” ê³³ì´ í•´ìˆ˜ë©´ ìƒìŠ¹ìœ¼ë¡œ ê°€ë¼ì•‰ëŠ”ë‹¤ë©´?")
st.caption("ê³µê°œ ë°ì´í„°(ê¸€ë¡œë²Œ NOAA ë“±) â†’ í•œêµ­ ê´€ì¸¡ì†Œ(1991-2020, 21ê°œ) ìˆœìœ¼ë¡œ ëŒ€ì‹œë³´ë“œ í‘œì‹œí•©ë‹ˆë‹¤. ëª¨ë“  ë¼ë²¨ì€ í•œêµ­ì–´ì…ë‹ˆë‹¤.")

# Pretendard í°íŠ¸ ì ìš© ì‹œë„ (ìˆìœ¼ë©´ ì ìš©, ì—†ìœ¼ë©´ ë¬´ì‹œ)
st.markdown("""
<style>
@font-face {
  font-family: 'Pretendard';
  src: url('/fonts/Pretendard-Bold.ttf') format('truetype');
  font-weight: 700;
  font-style: normal;
}
html, body, [class*="css"] { font-family: 'Pretendard', system-ui, -apple-system, "Segoe UI", Roboto, "Helvetica Neue", Arial; }
</style>
""", unsafe_allow_html=True)

TODAY = datetime.datetime.now().date()

# ------------------------
# HTTP í—¬í¼: ì¬ì‹œë„ ë¡œì§
# ------------------------
def requests_get_retry(url, max_retries=3, timeout=15):
    session = requests.Session()
    retries = Retry(total=max_retries, backoff_factor=0.8, status_forcelist=[429,500,502,503,504])
    session.mount("https://", HTTPAdapter(max_retries=retries))
    session.mount("http://", HTTPAdapter(max_retries=retries))
    r = session.get(url, timeout=timeout)
    r.raise_for_status()
    return r

# ------------------------
# ê³µê°œ ë°ì´í„°: ì‹œë„ì ìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°
# - 1) Global sea-level (DataHub NOAA/CSRIO mirror)
# - 2) NOAA/PSL monthly sea-level timeseries (fallback)
# - 3) NOAA OISST(í•´ìˆ˜ë©´ ì˜¨ë„) ìš”ì•½(ê°„ë‹¨)
# ------------------------
@st.cache_data(show_spinner=False)
def fetch_public_sea_level():
    # ì‹œë„ ìˆœì„œ: DataHub raw CSV -> NOAA PSL CSV -> ì‹¤íŒ¨ì‹œ None
    sources = [
        ("DataHub sea-level (CSV mirror)", "https://datahub.io/core/sea-level-rise/r/sea-level.csv"),
        ("NOAA PSL Sealevel (CSV)", "https://psl.noaa.gov/data/timeseries/month/SEALEVEL/sealevel.monthly.mean.csv"),
        ("NOAA climate.gov derived (CSV)", "https://www.climate.gov/sites/default/files/global_mean_sea_level.csv")
    ]
    last_err = None
    for name, url in sources:
        try:
            r = requests_get_retry(url)
            text = r.text
            return {"text": text, "url": url, "name": name}
        except Exception as e:
            last_err = e
            continue
    return None

@st.cache_data(show_spinner=False)
def fetch_noaa_sst_sample():
    # NOAA OISST í˜ì´ì§€ëŠ” ê·¸ë¦¬ë“œì¸ë° ì—¬ê¸°ì„  ê°„ë‹¨íˆ ë©”íƒ€ì •ë³´ë§Œ ê°€ì ¸ì˜¤ê³ , ì‹¤íŒ¨ì‹œ None
    url = "https://psl.noaa.gov/data/gridded/data.noaa.oisst.v2.highres.html"
    try:
        r = requests_get_retry(url)
        return {"text": r.text, "url": url}
    except:
        return None

public_sea = fetch_public_sea_level()
public_sst = fetch_noaa_sst_sample()

# ------------------------
# ê³µê°œ ë°ì´í„° ì²˜ë¦¬ (í‘œì¤€í™”)
# ê¸°ëŒ€: DataHub sea-level CSV í˜•íƒœ -> columns: 'Year','Month','GMSL' ë˜ëŠ” date-like
# ------------------------
def parse_sea_csv(text_blob):
    # ë‹¤ì–‘í•œ í¬ë§· ì‹œë„ë¥¼ í•´ì„œ date,value í‘œì¤€í˜• ë°˜í™˜
    try:
        df = pd.read_csv(StringIO(text_blob))
    except Exception:
        # ì‹œë„: skip bad lines
        df = pd.read_csv(StringIO(text_blob), error_bad_lines=False)
    # í›„ë³´ ì»¬ëŸ¼ íƒìƒ‰
    cols = [c.lower() for c in df.columns]
    # ì¼€ì´ìŠ¤ë³„ ì²˜ë¦¬
    # 1) 'Time' or 'time' + 'GMSL' or 'gmsl'
    if any('time' in c for c in cols) and any('gmsl' in c for c in cols):
        # find columns
        time_col = [c for c in df.columns if 'time' in c.lower()][0]
        val_col = [c for c in df.columns if 'gmsl' in c.lower()][0]
        df2 = df[[time_col, val_col]].rename(columns={time_col:'date', val_col:'value'})
        # try parse date
        df2['date'] = pd.to_datetime(df2['date'], errors='coerce')
        df2['value'] = pd.to_numeric(df2['value'], errors='coerce')
        return df2.dropna(subset=['date']).sort_values('date').reset_index(drop=True)
    # 2) ì›”ë³„ í‘œê°€ Year, Jan..Dec -> convert to long
    month_cols = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
    if 'Year' in df.columns or 'year' in cols:
        year_col = [c for c in df.columns if c.lower()=='year'][0] if 'Year' in df.columns or 'year' in cols else df.columns[0]
        available_months = [m for m in month_cols if m in df.columns]
        records = []
        for _, row in df.iterrows():
            try:
                year = int(row[year_col])
            except:
                continue
            for i, mon in enumerate(available_months, start=1):
                raw = row.get(mon, np.nan)
                try:
                    val = float(raw)
                except:
                    val = np.nan
                try:
                    date = pd.to_datetime(f"{year}-{i:02d}-01")
                except:
                    continue
                records.append({'date': date, 'value': val})
        if records:
            df_long = pd.DataFrame.from_records(records)
            return df_long.sort_values('date').reset_index(drop=True)
    # 3) Fall back: try to find first two columns: date,value
    if df.shape[1] >= 2:
        df2 = df.iloc[:, :2].copy()
        df2.columns = ['date','value']
        df2['date'] = pd.to_datetime(df2['date'], errors='coerce')
        df2['value'] = pd.to_numeric(df2['value'], errors='coerce')
        return df2.dropna(subset=['date']).sort_values('date').reset_index(drop=True)
    return pd.DataFrame(columns=['date','value'])

# ê³µê°œ ë°ì´í„° íŒŒì‹±
using_public_example = False
if public_sea is None:
    using_public_example = True
    st.warning("ê³µê°œ ë°ì´í„°(ê¸€ë¡œë²Œ í•´ìˆ˜ë©´) ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ â€” ì˜ˆì‹œ ë°ì´í„°ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
    # ê°„ë‹¨ ì˜ˆì‹œ: ì—°ë„ë³„ ëˆ„ì (mm)
    example = """date,value
1993-01-01,0.0
1995-01-01,5.2
2000-01-01,10.8
2005-01-01,18.0
2010-01-01,34.5
2015-01-01,45.1
2020-01-01,60.2
2023-01-01,72.4
"""
    public_df = parse_sea_csv(example)
else:
    public_df = parse_sea_csv(public_sea['text'])
    # remove future dates (ì˜¤ëŠ˜ ë¡œì»¬ ìì • ì´í›„ ë°ì´í„° ì œê±°)
    public_df = public_df[public_df['date'].dt.date <= TODAY].reset_index(drop=True)
    if public_df.empty:
        using_public_example = True
        st.warning("ê³µê°œ ë°ì´í„° íŒŒì‹± í›„ ìœ íš¨í•œ ì‹œê³„ì—´ì´ ì—†ì–´ ì˜ˆì‹œ ë°ì´í„°ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
        example = """date,value
1993-01-01,0.0
1995-01-01,5.2
2000-01-01,10.8
2005-01-01,18.0
2010-01-01,34.5
2015-01-01,45.1
2020-01-01,60.2
2023-01-01,72.4
"""
        public_df = parse_sea_csv(example)

# ì „ì²˜ë¦¬: ê²°ì¸¡ ë³´ê°„/ì¤‘ë³µ ì œê±°/ì •ë ¬
def preprocess_standard_ts(df):
    df = df.copy()
    df['date'] = pd.to_datetime(df['date'])
    df = df.drop_duplicates(subset=['date']).sort_values('date').reset_index(drop=True)
    df['value'] = pd.to_numeric(df['value'], errors='coerce')
    if df['value'].isna().any():
        df['value'] = df['value'].interpolate(method='time', limit_direction='both')
    # remove future dates (again, safety)
    df = df[df['date'].dt.date <= TODAY].reset_index(drop=True)
    return df

public_df = preprocess_standard_ts(public_df)

# ------------------------
# í•œêµ­(êµ­ë¦½í•´ì–‘ì¡°ì‚¬ì›) 21ê°œ ê´€ì¸¡ì†Œ ë°ì´í„° ì‹œë„ ê°€ì ¸ì˜¤ê¸° (MOF ë³´ë„ìë£Œ PDF)
# - ì¶œì²˜: https://www.mof.go.kr/doc/ko/selectDoc.do?docSeq=44140
# - ë°©ë²•: PDF ë‚´ í‘œ ì¶”ì¶œ ì‹œë„(poor man's approach). ì‹¤íŒ¨ ì‹œ ì˜ˆì‹œ ë°ì´í„° ì‚¬ìš©.
# ------------------------
@st.cache_data(show_spinner=False)
def fetch_korean_mof_pdf():
    pdf_page = "https://www.mof.go.kr/jfile/readDownloadFile.do?fileNum=1&fileType=MOF_ARTICLE&fileTypeSeq=44140"
    try:
        r = requests_get_retry(pdf_page)
        return {"bytes": r.content, "url": pdf_page}
    except Exception:
        return None

korea_pdf = fetch_korean_mof_pdf()
using_korea_example = False

def parse_mof_pdf_to_df(pdf_bytes):
    # ì‹œë„ì ìœ¼ë¡œ PDFì—ì„œ '21ê°œ' ê´€ë ¨ í‘œë¥¼ ì¶”ì¶œ. (í™˜ê²½ì— ë”°ë¼ ì‹¤íŒ¨í•  ìˆ˜ ìˆìŒ)
    try:
        with pdfplumber.open(BytesIO(pdf_bytes)) as pdf:
            all_tables = []
            for p in pdf.pages:
                tables = p.extract_tables()
                for t in tables:
                    # í…Œì´ë¸”ì€ ë¦¬ìŠ¤íŠ¸ í˜•íƒœ. í•©ì¹˜ê¸° ìœ„í•´ DFë¡œ ë³€í™˜ ì‹œë„.
                    df_t = pd.DataFrame(t[1:], columns=t[0]) if len(t) >=2 else None
                    if df_t is not None:
                        all_tables.append(df_t)
            if not all_tables:
                return None
            combined = pd.concat(all_tables, ignore_index=True)
            # ë‹¨ìˆœíˆ ë¦¬í„´ (í›„ì†ì—ì„œ í‘œì¤€í™” ì‹œë„)
            return combined
    except Exception:
        return None

korea_df_raw = None
if korea_pdf:
    korea_df_raw = parse_mof_pdf_to_df(korea_pdf['bytes'])
if korea_df_raw is None:
    using_korea_example = True
    st.info("í•œêµ­ ê´€ì¸¡ì†Œ(1991-2020) ë°ì´í„°: ì›ë¬¸(PDF)ì—ì„œ í‘œ ì¶”ì¶œì„ ì‹œë„í–ˆìœ¼ë‚˜ ì‹¤íŒ¨í•˜ê±°ë‚˜ í‘œ í˜•ì‹ì´ ë‹¤ì–‘í•˜ì—¬ ë‚´ì¥ ì˜ˆì‹œ ë°ì´í„°ë¡œ ë³´ì—¬ë“œë¦½ë‹ˆë‹¤.")
    # ì˜ˆì‹œ: 21ê°œ ê´€ì¸¡ì†Œ ì¤‘ ì¼ë¶€ë¥¼ ìƒ˜í”Œë¡œ ë§Œë“  ë°ì´í„° (ì—°-ì›”-ê´€ì¸¡ì†Œ-ìƒìŠ¹ë¥ (mm))
    korea_example = """date,station,value,region
1991-01-01,ìš¸ë¦‰ë„,2.3,ë™í•´
1995-01-01,í¬í•­,2.9,ë™í•´
2000-01-01,ë³´ë ¹,3.1,ì„œí•´
2005-01-01,ì¸ì²œ,3.4,ì„œí•´
2010-01-01,ì†ì´ˆ,3.8,ë™í•´
2015-01-01,ìš¸ë¦‰ë„,4.9,ë™í•´
2020-01-01,í¬í•­,6.17,ë™í•´
"""
    korea_df = pd.read_csv(StringIO(korea_example))
else:
    # ë‹¨ìˆœ í‘œì¤€í™” ì‹œë„: ì»¬ëŸ¼ëª…ì´ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ 'date' ë˜ëŠ” 'ì—°ë„'ë“±ìœ¼ë¡œ ë³€í™˜
    dfk = korea_df_raw.copy()
    # ì‹œë„ì ìœ¼ë¡œ ì—°ë„/ì›”/ê°’ ì»¬ëŸ¼ ì°¾ê¸°
    cols = [c.strip() for c in dfk.columns]
    lower = [c.lower() for c in cols]
    # find year/month or date
    date_col = None
    if any('date' in c for c in lower):
        date_col = cols[lower.index([c for c in lower if 'date' in c][0])]
    elif any('ì—°' in c or 'year' in c for c in lower):
        date_col = cols[0]
    # find value-like column
    value_col = None
    for candidate in ['ê°’','í•´ìˆ˜ë©´','ìƒìŠ¹','mm','rate','value']:
        matches = [c for c in lower if candidate in c]
        if matches:
            value_col = cols[lower.index(matches[0])]
            break
    # station
    station_col = None
    for candidate in ['ê´€ì¸¡','station','ì§€ì ','ê´€ì¸¡ì†Œ','site','location']:
        matches = [c for c in lower if candidate in c]
        if matches:
            station_col = cols[lower.index(matches[0])]
            break
    # try build minimal df
